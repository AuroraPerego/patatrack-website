---
title: "Patatrack goes to Australia"
author: "Felice Pantaleo"
layout: default
categories: announcements
# announcement valid from 
date: 01/11/2019
# announcement valid until
until: 22/07/2020

---


# {{page.title}}

The International Conference on Computing in High Energy and Nuclear Physics is one of the biggest events for software geeks like us. Its 24th Edition was just held from the 4th to the 8th of November 2019 in Adelaide, South Australia.
The Patatrack Team could not miss the opportunity to present some new and exciting results. 

![FeliceAndreaAdelaide2019]({{site.baseurl}}/images/CHEP19/FeliceAndreaAdelaide2019.jpeg){:height="400px"}


## New results from Patatrack Team

We are excited to announce new results from the physics and computational points of view concerning the Patatrack Pixel Reconstruction. 

### Physics results

The pixel reconstruction was improved by accounting for the geometric acceptance in the Pattern Recognition step and allowing a looser requirement on the number of hits belonging to a track. Instead of requiring at least four hits, now it is possible to go down to at least three hits per n-tuplet. 
This allows to improve the efficiency, as shown for single muon events without any pileup collision: 

![singleMu_noPU_compareAll_efficiency_eta]({{site.baseurl}}/images/CHEP19/singleMu_noPU_compareAll_efficiency_eta.png){:height="400px"}

This new reconstruction was tested also for ttbar events with an average number of simultaneous proton-proton collisions of *50*.

![ttbar_pu50_compareAll_efficiency_eta]({{site.baseurl}}/images/CHEP19/ttbar_pu50_compareAll_efficiency_eta.png){:height="400px"}
![ttbar_pu50_compareAll_efficiency_pt]({{site.baseurl}}/images/CHEP19/ttbar_pu50_compareAll_efficiency_pt.png){:height="400px"}
![ttbar_pu50_compareAll_fakerate_FromPV_eta]({{site.baseurl}}/images/CHEP19/ttbar_pu50_compareAll_fakerate_FromPV_eta.png){:height="400px"}
![ttbar_pu50_compareAll_fakerate_FromPV_pt]({{site.baseurl}}/images/CHEP19/ttbar_pu50_compareAll_fakerate_FromPV_pt.png){:height="400px"}

From the computational point of view, the big news is that the same CUDA code can now execute with the same results on both GPU and CPU as shown in the plot below:

![ttbar_pu50_CPU_GPU_compareAll_efficiency_eta]({{site.baseurl}}/images/CHEP19/ttbar_pu50_CPU_GPU_compareAll_efficiency_eta.png){:height="400px"}

With all the ingredients above we were able to measure the throughput of both the GPU and CPU versions of the Patatrack Reconstruction when producing tracks from at least three (triplets) or four hits (quadruplets), and compare it against the CMS Pixel Reconstruction used for data taking in 2018.
We compare a full node powered by a dual socket Xeon Gold 6130 (2 × 16 cores) running 4 CMSSW Processes, each with 16 threads, against an NVIDIA Tesla T4. For the GPU benchmark we measure the throughput in three configurations: when leaving the result on the GPU to be used by another GPU-based consumer (green), when copying back the result to the host in a Structure-of-Array data format (blue) and when converting the Structure-of-Arrays to the legacy data format (red). 

![throughput]({{site.baseurl}}/images/CHEP19/throughput.png){:height="400px"}

The Patatrack Reconstruction running on a NVIDIA T4 can outperform the existing CMS reconstruction executed on an entire dual Xeon Gold "Skylake" node by a factor 2 when running quadruplets, and by 10% when running triplets (with much better physics performance).
The Patatrack reconstruction running on CPU can achieve the same computational performance of the CMS 2018 reconstruction, but using way more complex algorithms and producing much improved tracks.


## Combining ARM Processors with NVIDIA GPUs

About one year ago, we started investigating the possibility to run the Patatrack Reconstruction on ARM+GPU. For this reason we acquired an NVIDIA Jetson AGX Xavier Developer Kit. It features an ARMv8.2 8-cores CPU together with a small slice of a Volta GPU (512 CUDA cores) in a low-power envelop. 
By using Linux Containers, we managed to get the CMS Software ([CMSSW](https://github.com/cms-sw/cmssw/)) to run smoothly on this device. 
The throughput performance of the Patatrack Reconstruction are shown in the plot below:

![XavierT4]({{site.baseurl}}/images/CHEP19/XavierT4.png){:height="400px"}

In our benchmarks the Xavier Kit was configured in two modes:
* MAXN mode: no power limit for the envelop,
* max 30W mode: all CPU cores are active with reduced clock.

A Xavier can achieve about a fourth of the throughput of a T4 installed on a Intel Xeon CPU.

### World Premiere Patatrack Reconstruction running on ARM and NVIDIA V100

Thanks to a collaboration with NVIDIA and the Oak Ridge Leadership Computing Facility, we were able to access and use for the first time a preproduction system, featuring two Cavium ThunderX2 CPUs and two NVIDIA Tesla V100 GPUs:
* 2 × Cavium ThunderX2 CN9975: 28 cores, 112 threads, 150-170 W power envelope;
* 2 × NVIDIA Tesla V100: 5120 CUDA cores, 250 W power envelope.

Software-wise, the system runs:
* CentOS 8.0 with Linux kernel 4.18.0-80 and GCC 8.3.1;
* pre-production CUDA 10.2.91 with NVIDIA drivers 435.17.01.

After building for ARMv8 and CentOS 8 the CMS Software with the GPU-accelerated Patatrack reconstruction, so it could run natively on this system, we compare the throughput in reconstructed events per second of the Cavium ThunderX2 against the following machine:
* 2 × Intel Xeon Silver 4114: 10 cores, 20 threads, 85 W power envelope;
* 1 × NVIDIA Tesla V100: 5120 CUDA cores, 250 W power envelope.

We run 4 CMSSW processes per GPU and in each job we measure the throughput for a different number of concurrent events (EDM Streams). 

At the plateau the Cavium ThunderX2 can process about *1737* events per second per GPU, while the Intel Silver 4114 with the single V100 reaches about *1800* events per second.

![WombatVsIntel]({{site.baseurl}}/images/CHEP19/WombatVsIntel.png){:height="400px"}

This is the first time an ARM based server is coupled to a discrete GPU. Performance-wise, the results are equivalent to an x86 solution, opening new and exciting opportunities for computing in High Energy Physics. 

#### Acknowledgments
NVIDIA and OLCF for providing us with the preproduction machine and software that we used for our tests.

Our colleagues from the CMS collaboration who made our work much easier by maintaining an ARM build of the CMS Software.

The CMS Open Data group for preparing and releasing the [simulated data](http://opendata.cern.ch/record/12303) used in this work.
